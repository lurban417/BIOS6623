---
title: "Project 3"
author: "Lauren Urban"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Read In and Summary Statistics
```{r}

# Load libraries
library(dplyr)
library(caret)

# Load CSV 
data <- read.csv("/Users/laurenurban/Downloads/Project3_data.csv")

# View the first few rows 
head(data)

# Summary statistics
str(data)
summary(data)
```
# Split data into training and test sets
```{r}
# Split into training and test sets
set.seed(123)  
trainIndex <- createDataPartition(data$progression, p = 0.6, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Check proportions of progression in each set to make sure they are ~ equal
train_proportion <- prop.table(table(train_data$progression))
test_proportion <- prop.table(table(test_data$progression))

cat("Proportion of progression in training set:\n")
print(train_proportion)
cat("Proportion of progression in test set:\n")
print(test_proportion)
```

# Square baseline kidney volume
```{r}

# Square baseline kidney volume 
train_data <- train_data %>%
  mutate(tkvht_base_squared = tkvht_base^2)

test_data <- test_data %>%
  mutate(tkvht_base_squared = tkvht_base^2)
```

# Z score normalize all variables
```{r}

# Z score normalize all variables
preProc <- preProcess(train_data, method = c("center", "scale"))
train_data <- predict(preProc, train_data)
test_data <- predict(preProc, test_data)
```

# Cross validation 
```{r}

# Randomly split data into 5 folds for cross-validation
train_control <- trainControl(method = "cv", number = 5)
```

# Task 1 Models
```{r}
# Task 1

# Model 1: Baseline kidney volume only
predictors_model1 <- "tkvht_base"

# Model 2: Image features only
image_features <- grep("^(geom|gabor|glcm|txti|lbp)", colnames(data), value = TRUE)
predictors_model2 <- image_features

# Model 3: Baseline kidney volume + image features
predictors_model3 <- c("tkvht_base", image_features)

# Define a function to train and evaluate a linear regression model
evaluate_model <- function(train_data, test_data, predictors, outcome) {
  formula <- as.formula(paste(outcome, "~", paste(predictors, collapse = " + ")))
  model <- lm(formula, data = train_data)
  
  # Predict on test data
  predictions <- predict(model, newdata = test_data)
  
  # Calculate RMSE (Root Mean Square Error) for performance evaluation
  rmse <- sqrt(mean((test_data[[outcome]] - predictions)^2))
  
  list(model = model, rmse = rmse)
}

# Train and evaluate each model
outcome_var <- "tkvht_change"

# Model 1
model1 <- evaluate_model(train_data, test_data, predictors_model1, outcome_var)
cat("Model 1 RMSE (Baseline kidney volume only):", model1$rmse, "\n")

# Model 2
model2 <- evaluate_model(train_data, test_data, predictors_model2, outcome_var)
cat("Model 2 RMSE (Image features only):", model2$rmse, "\n")

# Model 3
model3 <- evaluate_model(train_data, test_data, predictors_model3, outcome_var)
cat("Model 3 RMSE (Baseline kidney volume + Image features):", model3$rmse, "\n")

# Compare RMSE values
rmse_values <- data.frame(
  Model = c("Baseline kidney volume only", "Image features only", "Baseline + Image features"),
  RMSE = c(model1$rmse, model2$rmse, model3$rmse)
)

print(rmse_values)

# Define a function to train and evaluate a logistic regression model
evaluate_classification_model <- function(train_data, test_data, predictors, outcome) {
  formula <- as.formula(paste(outcome, "~", paste(predictors, collapse = " + ")))
  model <- glm(formula, data = train_data, family = binomial)
  
  # Predict on test data
  test_probs <- predict(model, newdata = test_data, type = "response")
  test_preds <- ifelse(test_probs > 0.5, 1, 0)
  
  # Calculate accuracy for performance evaluation
  accuracy <- mean(test_preds == test_data[[outcome]])
  
  list(model = model, accuracy = accuracy)
}
```
# Task 2 Models
```{r}
# Ensure the `progression` column is binary (0 for slow, 1 for fast progression)
# Modify if needed based on your data structure:
train_data$progression <- ifelse(train_data$progression == "fast", 1, 0)
test_data$progression <- ifelse(test_data$progression == "fast", 1, 0)

# Model 1: Baseline kidney volume only
predictors_model1 <- "tkvht_base"

# Model 2: Image features only
predictors_model2 <- image_features

# Model 3: Baseline kidney volume + image features
predictors_model3 <- c("tkvht_base", image_features)

# Define a function to train and evaluate a logistic regression model
evaluate_classification_model <- function(train_data, test_data, predictors, outcome) {
  # Define the formula
  formula <- as.formula(paste(outcome, "~", paste(predictors, collapse = " + ")))
  
  # Fit logistic regression model
  model <- glm(formula, data = train_data, family = binomial)
  
  # Predict probabilities on test data
  test_probs <- predict(model, newdata = test_data, type = "response")
  
  # Convert probabilities to binary predictions
  test_preds <- ifelse(test_probs > 0.5, 1, 0)
  
  # Calculate accuracy
  accuracy <- mean(test_preds == test_data[[outcome]])
  
  list(model = model, accuracy = accuracy)
}

# Define the outcome variable
outcome_var <- "progression"

# Model 1: Baseline kidney volume only
model1 <- evaluate_classification_model(train_data, test_data, predictors_model1, outcome_var)
cat("Model 1 Accuracy (Baseline kidney volume only):", model1$accuracy, "\n")

# Model 2: Image features only
model2 <- evaluate_classification_model(train_data, test_data, predictors_model2, outcome_var)
cat("Model 2 Accuracy (Image features only):", model2$accuracy, "\n")

# Model 3: Baseline kidney volume + Image features
model3 <- evaluate_classification_model(train_data, test_data, predictors_model3, outcome_var)
cat("Model 3 Accuracy (Baseline kidney volume + Image features):", model3$accuracy, "\n")

# Compare accuracy values in a data frame
accuracy_values <- data.frame(
  Model = c("Baseline kidney volume only", "Image features only", "Baseline + Image features"),
  Accuracy = c(model1$accuracy, model2$accuracy, model3$accuracy)
)

print(accuracy_values)
```